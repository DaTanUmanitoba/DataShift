# -*- coding: utf-8 -*-
"""Descriptive.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xCP6X4yriXJ96W6b0BTFFI11a0C8wuEL

**Descriptive analysis**

from google.colab import files
#upload the final_combined_toy.csv
files.upload()

#upload the split_reviewYear2010.csv
files.upload()

#upload the split_reviewYear2015_reviewMonth04.csv
files.upload()

#upload the split_reviewYear2017_reviewMonth01_categoryBooks.csv
files.upload()
"""

def read_data(file_path):
  return pd.read_csv(file_path,
            dtype={
            "reviewMonth": str,
            "unixReviewTime": int,
            "verified": bool,
            "category": str,
            "reviewYear": str, #or treat it as an int??
            "score": str
            },
            )

import pandas as pd
all_data = read_data("final_combined_toy.csv")
year_data = read_data("split_reviewYear2010.csv")
month_data = read_data("split_reviewYear2015_reviewMonth04.csv")
category_data = read_data("split_reviewYear2017_reviewMonth01_categoryBooks.csv")

selected_cat = ['Books', 'Electronics', 'Movies_and_TV', 'Home_and_Kitchen']
month_data2 = month_data[month_data['category'].isin(selected_cat)]
#month_data[ [x for x in range(len(month_data.index)) if month_data.ix[x,'category'] in selected_cat],:]
contTab = pd.crosstab(index=month_data2['score'], columns=month_data2['category'])
print(contTab)

"""**chi-squre test for each category variables**

For the year: it is significant: $p<0.05$, indicating distribution shift with the review year

For the month, we pick a specific year and compute the chi-square, which is not significant, also not significant for the whole toy dataset

For the category, we pick a specific year-month, and the chi-square is not significant, also I pick four big categories and the statististic is still not significant; however, it is significant using the whole dataset; So, we need to know whether this phenomenon is prevalent for all year+month sub dataset.


"""

from scipy.stats import chi2_contingency
from scipy.stats import chi2
from scipy import stats
import numpy as np

#print(all_data.columns)
def print_chi2(data, col_name):
  stat, p, dof, expected = chi2_contingency( pd.crosstab( index=data['score'], columns=data[col_name] ) )
  print("chi_statistic=%d" % stat)
  print('degree_of_freedom=%d' % dof)
  #print(expected)
  # interpret test-statistic
  prob = 0.95
  critical = chi2.ppf(prob, dof)
  print('probability=%.3f, critical=%.3f, stat=%.3f' % (prob, critical, stat))
  if abs(stat) >= critical:
    print('Dependent (reject H0)')
  else:
    print('Independent (fail to reject H0)')
  # interpret p-value
  alpha = 1.0 - prob
  print('significance threshold=%.3f, p=%.3f' % (alpha, p))
  if p <= alpha:
    print('Dependent (reject H0)')
  else:
    print('Independent (fail to reject H0)')
  return p

#for the feature "reviewYear", "reviewMonth", and "category"
print_chi2(all_data, "reviewYear")
print_chi2(year_data, "reviewMonth")
print_chi2(month_data2, "category")
print_chi2(all_data, "category")

"""Next we do boxplot for each numeric feature and summary for its statistics

Also we do shaprio test: no feature has normal distribution in the year dataset

Also we do correlation matrix

Also we do aggregate statistics, by year the mean values for aveP and aveUser

Then we do feature selection based on ANOVA f-test

Also we do feature importance ranking using RF

statistics summary

All fields are not normal-distributions
"""

#shaprio test, summary statistics, and box plotting
def statSummary(data):
  numericColIndex = range(18)
  data = data.iloc[:,numericColIndex]
  print(data.describe())

  for ind in numericColIndex:
    shapiro_test = stats.shapiro(np.array(data.iloc[:,ind]))
    print("The shaprio test pvalue for %s is %f." % ( data.columns[ind], shapiro_test[1] ) )

statSummary(year_data)

def setup_plot():
  plt.rcParams["axes.grid.axis"] ="y"
  plt.rcParams["axes.grid"] = True
  plt.rcParams["legend.fontsize"] = 14
  plt.rc('grid', linestyle="dashed", color='lightgrey', linewidth=1)
  plt.rcParams["xtick.labelsize"] = 8
  plt.rcParams["ytick.labelsize"]  = 8
  plt.xticks(rotation=45, ha="right")

"""Next, we plot violin quartile plots for each numeric feature; each feature is 0-1 scaled"""

import matplotlib.pyplot as plt
import matplotlib
import seaborn as sns
from matplotlib.pyplot import figure
from sklearn.preprocessing import MinMaxScaler

scaler = MinMaxScaler()
figure(figsize=(24, 12), dpi=180)

#violin plotting for each numeric feature
def plotViolin(data):
  df = data.copy()
  df = df.iloc[:, range(18)]
  df = pd.DataFrame( scaler.fit_transform(df), columns=df.columns )
  print(df.iloc[1:3,:])
  #df = df.reset_index()
  df = pd.melt(df, value_vars=df.columns[range(18)])
  print(df.iloc[1:3,:])
  ax = sns.violinplot(x="variable", y="value", data=df, inner="quartile")

setup_plot()
plotViolin(category_data)

"""correlation matrix heatmap among the numeric features

It shows that some features are highly correlated, indication redundence of the features
"""

#
def plotCorr(data):
  df = data.copy()
  df = df.iloc[:, range(18)]
  df_cor = df.corr()
  print("The correlation matrix is:")
  figure(figsize=(15, 10), dpi=100)
  ax = sns.heatmap(df_cor, vmin=-1, vmax=1, annot=True)
  ax.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)

setup_plot()
plotCorr(category_data)

"""aggregate statistics: The mean and median of avgProduct and avgUser for each year

"""

#titanic[["Sex", "Age"]].groupby("Sex").mean()
def print_aggregation(data):
  result = data[["avgProductScore", "avgUserScore", "reviewYear"]].groupby("reviewYear").mean()
  print(result)
  result = data[["avgProductScore", "avgUserScore", "reviewYear"]].groupby("reviewYear").median()
  print(result)

#print(all_data.columns)
print_aggregation(all_data)

"""Then we do feature selection based on ANOVA f-test;
The analysis suggests three useful features: D1, avgProductScore; avgUserScore
"""

from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import f_classif
from sklearn.feature_selection import mutual_info_classif
from matplotlib import pyplot

# feature selection
def select_features(X_train, y_train):
	# configure to select all features
	#fs = SelectKBest(score_func=mutual_info_classif, k='all')# mutual_info method does not compute the pvalue
  fs = SelectKBest(score_func=f_classif, k='all')
	# learn relationship from training data
  fs.fit(X_train, y_train)
	# transform train input data
  X_train_fs = fs.transform(X_train)
  return X_train_fs, fs

# feature selection
print(category_data.columns)
x_train = category_data.iloc[:,range(18)]
y_train = category_data.iloc[:,23]
y_train = y_train.astype("category")
#print(y_train)

X_train_fs, fs = select_features(x_train, y_train)
#print(fs.pvalues_)
# what are scores for the features
for i in range(len(fs.scores_)):
 print('Feature %d: with f-1 score and pvalue %f %f' % (i, fs.scores_[i], fs.pvalues_[i]))
# plot the scores
#pyplot.bar([i for i in range(len(fs.scores_))], fs.scores_)
pyplot.bar([i for i in range(len(fs.pvalues_))], fs.pvalues_)
pyplot.show()


"""Also we do feature importance ranking using RF Boruta:

all relevant features will be selected, even if some features may be redundent
We found that the algorithm only supports the last two features

Algorithm Boruta: https://github.com/scikit-learn-contrib/boruta_py

"""

from sklearn.ensemble import RandomForestClassifier
from boruta import BorutaPy

# define random forest classifier, with utilising all cores and
# sampling in proportion to y labels
rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced') #, max_depth=5)

# define Boruta feature selection method
feat_selector = BorutaPy(rf, n_estimators='auto', verbose=0, random_state=1) #verbose=2: details

X = x_train.values
y = y_train.values.ravel()
# find all relevant features using Boruta
feat_selector.fit(X, y)

# check selected features 
print(feat_selector.support_)

# check ranking of features
print(feat_selector.ranking_)

# call transform() on X to filter it down to selected features
X_filtered = feat_selector.transform(X)

"""In conclusion, the relation bwteen the review text vetors and the score is not strong in our test data "split_reviewYear2017_reviewMonth01_categoryBooks"

We will wrap these analysis into modules and test other split file as well as the whole file
"""

x_train = all_data.iloc[:,range(18)]
y_train = all_data.iloc[:,23]
y_train = y_train.astype("category")

X = x_train.values
y = y_train.values.ravel()
# find all relevant features using Boruta
feat_selector.fit(X, y)
# check selected features 
print(feat_selector.support_)
# check ranking of features
print(feat_selector.ranking_)
# call transform() on X to filter it down to selected features
X_filtered = feat_selector.transform(X)

"""It takes 6 minutes to finish, the result suggest that the Boruta-spported features are: D1, avgProduct, avgUser"""